{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from quality_classifier.embedding_models import generate_dino_embedding_model, generate_siglip_embedding_model, load_ziped_images\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Object informations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6219"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_car_quality_votes = pd.read_csv(\n",
    "    '../data/car_quality_dataset_votes.csv')\n",
    "len(df_car_quality_votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings of Multi-View Renderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_extracted_images(object_dir):\n",
    "    all_files = os.listdir(object_dir)\n",
    "    object_images = [Image.open(os.path.join(object_dir, file_name)).convert(\"RGB\") for file_name in all_files if file_name.endswith(\".png\")]\n",
    "    return object_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ziped = False\n",
    "# render_base_path = \"../02_multi-view-rendering/car_renders_21\"\n",
    "render_base_path = \"/mnt/damian/.objaverse/car_renders_new\"\n",
    "all_object_dirs = os.listdir(render_base_path)\n",
    "car_uids = df_car_quality_votes['uid'].unique()\n",
    "rendered_image_dict = {}\n",
    "for uid in car_uids:\n",
    "    if uid in rendered_image_dict:\n",
    "        continue\n",
    "    if is_ziped:\n",
    "        filename = uid + \".zip\"\n",
    "        if filename in all_object_dirs:\n",
    "            object_dir = os.path.join(render_base_path, filename)\n",
    "            object_images = load_ziped_images(object_dir)\n",
    "    else:\n",
    "        object_dir = os.path.join(render_base_path, uid)\n",
    "        if os.path.exists(object_dir):\n",
    "            object_images = load_extracted_images(object_dir)\n",
    "    rendered_image_dict[uid] = object_images \n",
    "\n",
    "len(rendered_image_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep the uids that are in the rendered_image_dict\n",
    "df_car_quality_votes = df_car_quality_votes[df_car_quality_votes['uid'].isin(\n",
    "    rendered_image_dict.keys())]\n",
    "len(df_car_quality_votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding the images using DINO or Siglip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"DINOv2\" # \"DINOv2\"\n",
    "device = \"cuda\"\n",
    "\n",
    "if embedding_model == \"siglip\":\n",
    "    embed = generate_siglip_embedding_model(device=device)\n",
    "    use_clip = True\n",
    "elif embedding_model == \"colpali\":\n",
    "    embed = generate_colpali_embedding_model(device=device)\n",
    "    use_clip = False\n",
    "else: # \"DINOv2\"\n",
    "    embed = generate_dino_embedding_model(device=device)\n",
    "    use_clip = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through all images and estimate the embedding using the selected model. All embeddings will be saved with their corresponding uid in a dictionary.\n",
    "> Note that this can take a while "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(embedding_fnc):\n",
    "    all_embeddings = {}\n",
    "    uids = list(rendered_image_dict.keys())\n",
    "    # image preprocessing\n",
    "    for uid in tqdm(uids):\n",
    "        images = rendered_image_dict[uid]\n",
    "        # resize all images to 576x576\n",
    "        images = [image.resize((576, 576)) for image in images]\n",
    "        images = [image for image in images]\n",
    "        # generate the embeddings \n",
    "        outputs = embedding_fnc(images)\n",
    "        embeddings = outputs.cpu().numpy()\n",
    "        # print(embeddings.shape)\n",
    "        all_embeddings[uid] = embeddings # .reshape(embeddings.shape[0]*embeddings.shape[1])\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "concat_images = False\n",
    "only_one_image_per_uid = True\n",
    "amount_of_embeddings = 1\n",
    "\n",
    "embedding_dataset = []\n",
    "# first generate the embeddings without color jitter\n",
    "all_embeddings = generate_embeddings(embed)\n",
    "for uid, embedding in all_embeddings.items():\n",
    "    vote = df_car_quality_votes[df_car_quality_votes['uid'] == uid]['vote'].values[0]\n",
    "    embedding_dataset.append((embedding, vote, uid))\n",
    "embedding_dataset = pd.DataFrame(embedding_dataset, columns=['embedding', 'vote', 'uid'])\n",
    "embedding_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del rendered_image_dict\n",
    "del embed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving embeddings\n",
    "safe all_embeddings into a hdf5 file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embeddings of embedding_dataset as a hdf5 file\n",
    "embeddings_list = embedding_dataset['embedding'].values\n",
    "# make single numpy array from array of arrays\n",
    "embeddings_np = np.stack(embeddings_list)\n",
    "print(f\"embeddings_np.shape: {embeddings_np.shape}\")\n",
    "filename = 'car_model_embedding_' + embedding_model + '_seq_4.h5'\n",
    "with h5py.File(filename, 'w') as f:\n",
    "    f.create_dataset('embedding_dataset', data=embeddings_np)\n",
    "print(\"Embeddings saved to \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the votes of embedding_dataset as a hdf5 file\n",
    "filename = 'car_model_votes_' + embedding_model + '_seq_4.h5'\n",
    "votes_list = embedding_dataset['vote'].values\n",
    "# make single numpy array from array of arrays\n",
    "votes_np = np.stack(votes_list)\n",
    "print(f\"votes_np.shape: {votes_np.shape}\")\n",
    "with h5py.File(filename, 'w') as f:\n",
    "    f.create_dataset('vote_dataset', data=votes_np)\n",
    "print(\"Votes saved to \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the uids of embedding_dataset as a hdf5 file\n",
    "filename = 'car_model_uids_' + embedding_model + '_seq_4.h5'\n",
    "uids_list = embedding_dataset['uid'].values\n",
    "with h5py.File(filename, 'w') as f:\n",
    "    f.create_dataset('uid_dataset', data=uids_list)\n",
    "print(\"Uids saved to \" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the embeddings to a pkl file and generate PCA Model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7239, 4, 197376)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "load_embeddings = True\n",
    "\n",
    "if load_embeddings:\n",
    "    embeddings_filename = '../data/car_model_embedding_DINOv2_seq_4.h5'\n",
    "    with h5py.File(embeddings_filename, 'r') as f:\n",
    "        embeddings = f['embedding_dataset'][:]\n",
    "else:\n",
    "    embeddings = embeddings_np\n",
    "\n",
    "if use_clip:\n",
    "    reduce_embeddings = False\n",
    "else:\n",
    "    reduce_embeddings = True\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressing the data to shape: 7239, 4, 768\n",
      "(7239, 3072)\n",
      "sum of explained variance ratio:  0.9200398354987556\n",
      "saved pca file\n",
      "all_embeddings_new.shape: (7239, 4, 768)\n"
     ]
    }
   ],
   "source": [
    "original_shape = embeddings.shape\n",
    "all_embeddings_new = embeddings.reshape(embeddings.shape[0], embeddings.shape[1]*embeddings.shape[2])\n",
    "# randomly sample 100000 embeddings from all_embeddings_new\n",
    "# all_embeddings_new = all_embeddings_new[np.random.choice(all_embeddings_new.shape[0], 20000, replace=False)]\n",
    "n_components = embeddings.shape[1]*768\n",
    "print(f\"compressing the data to shape: {embeddings.shape[0]}, {embeddings.shape[1]}, {768}\")\n",
    "\n",
    "if reduce_embeddings:\n",
    "    pca = PCA(n_components=n_components, random_state=22)\n",
    "    pca.fit(all_embeddings_new)\n",
    "    all_embeddings_new = pca.transform(all_embeddings_new)\n",
    "    print(all_embeddings_new.shape)\n",
    "    print(\"sum of explained variance ratio: \", sum(pca.explained_variance_ratio_))\n",
    "    pca_file = 'pca_model_' + embedding_model + '_seq_4_do_jit_sn_2.pkl'\n",
    "    with open(pca_file, 'wb') as f:\n",
    "        pickle.dump(pca, f)\n",
    "    print(\"saved pca file\")\n",
    "    new_filename = 'car_model_embedding_' + embedding_model + '_seq_4_do_jit_sn_2_reduced.h5'\n",
    "    all_embeddings_new = all_embeddings_new.reshape(original_shape[0], original_shape[1], -1)\n",
    "    print(f\"all_embeddings_new.shape: {all_embeddings_new.shape}\")\n",
    "    with h5py.File(new_filename, 'w') as f:\n",
    "        f.create_dataset('embedding_dataset', data=all_embeddings_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "objaverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
